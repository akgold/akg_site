[
  {
    "path": "posts/2020-12-02-practical-package-patterns/",
    "title": "R Packages for Experts, not Wizards",
    "description": "Write internal R packages for your team that disseminate expertise, not cleverness.",
    "author": [],
    "date": "2021-01-22",
    "categories": [],
    "contents": "\nI, like so many other R aficionados, love writing R packages for my team. They’re a great way to get everyone on the same page and to ensure everyone has access to the same resources. But I’ve also (more than once) lovingly crafted an R package only to find that…no one used it.\nI’d incorrectly thought a bunch of clever R code could solve my team’s problems. I’d assumed I, the team’s R wizard, could go write the perfect package, and return to great acclaim that I’d fixed everyone’s woes.\nUnfortunately, my vision of how the package writing process was busted in at least three ways – ones that I’ve seen befall other teams and R aficionados. Read on for more.\nThe Wizard Package\nWhen I wrote my team’s packages, I was the R wizard on the team – and if you’re reading this, you probably are too. I believed I could go off, write a package, and deliver it from on high a la Moses from Mount Sinai.1\nI’ve come to call the result of this (not so great) pattern the Wizard Package.\nWizard Packages can work. I’ve seen Wizard Packages work when (1) the whole team is clear that a package will solve the problem, (2) the need is can be met by clever R code, and (3) the team is strongly motivated to adopt the package.\nIn my case, none of these conditions held. I wasn’t riding a wave of enthusiasm for R – I was trying to spark it. People weren’t super motivated to adopt the package, and I couldn’t really compel them to do so. Worst of all, my R code just wasn’t solving the keenest problems of my teammates.\nBecause they’re written by R wizards, Wizard Packages tend to solve R wizard problems. They have clever wrappers for other functions, or convenience functions for advanced R users.\nVery often though, the team’s needs aren’t needs that are solved primarily by clever R code – the team needs to get their R Markdown report formatted with the right CSS, or calculate correct standard errors from a complicated estimator, or complete those weird database merges that only one person on the team knows how to do.2\nWizard packages convey expertise of a sort, but they often aren’t adopted because they’re not expert enough, or aren’t expert in the right ways.\nExpert Packages\nR packages are a tool to make your team’s expertise real. By incorporating knowledge from everyone on the team, you can create a package that disseminates that expertise to everyone on the team.\nThere’s tons of psych research (that I’m too lazy to actually cite) that one of the best ways to get someone excited about something is to get their help with it.\nMore importantly, there’s far more to package development than writing R code, so the process really can include everyone, regardless of their level of comfort with R.\nA few ways for folks to help out who might not be the R wizards:\nDesigning the API Even folks who are novice R developers probably have a strong sense about what the important features and options are for things like plotting the data.\nDesigning the data model Someone who knows the problem really well probably has the best sense of what the important parts of the data are to keep track of.\nSharing expert knowledge An Expert Package shares expertise across the team. By collecting that knowledge directly, you can incorporate someone’s data vis, database access, or stats expertise without needing them to write R code.\nWriting documentation, tests, or vignettes Internal packages often include documentation or vignettes that go beyond just explaining the functions themselves, but providing some context on why the function works as it does. People can also provide feedback on whether package tests are meaningful.\nMake Your Packages Experts, not Wizards\nThere’s a reason that so many stories of Wizards include dark endings, evil omens, and fallen mages. Wizardry is ineherently disconnected from others, and it’s all about being clever.\nIn the land of R packages, being expert is so much more valuable than being clever – doubly so if you can provide expertise and value to others on your team, and the number one way to do that is to get your teammates involved in the package creation process the whole way along.\n\nIf only I’d remembered the end of that story, where Moses’s return isn’t exactly greeted with enthusiasm.↩︎\nEvery team has this person, and they are amazing. They also are all-too-often undervalued, because their skills aren’t as “technical” as the coding wizards. That is false.↩︎\n",
    "preview": "posts/2020-12-02-practical-package-patterns/images/wizard.png",
    "last_modified": "2021-07-07T19:47:13+00:00",
    "input_file": {},
    "preview_width": 488,
    "preview_height": 514
  },
  {
    "path": "posts/2019-10-17-production-shiny-w-pins/",
    "title": "RViews: Production Shiny Apps with Pins",
    "description": "Using the new pins package to create a shiny app.",
    "author": [
      {
        "name": "Alex Gold",
        "url": {}
      }
    ],
    "date": "2019-10-17",
    "categories": [],
    "contents": "\nRViews Post\n\n\n",
    "preview": {},
    "last_modified": "2021-07-07T19:47:13+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-07-07-r-big-data/",
    "title": "RViews: 3 Big Data Strategies for R",
    "description": "3 strategies for working with big data in R.",
    "author": [
      {
        "name": "Alex Gold",
        "url": {}
      }
    ],
    "date": "2019-07-07",
    "categories": [],
    "contents": "\nRViews Post\n\n\n",
    "preview": {},
    "last_modified": "2021-07-07T19:47:13+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-09-upgrading-to-r-3-be-the-data-engineer-you-need/",
    "title": "Upgrading to R #3: Be the Data Engineer you Need",
    "description": "Be the data engineer of your dreams.",
    "author": [
      {
        "name": "Alex Gold",
        "url": {}
      }
    ],
    "date": "2019-02-20",
    "categories": [],
    "contents": "\nWhen I first went to data conferences, I assumed that EVERYONE else was pulling data from a supercomputer-backed Spark cluster, immaculately maintained by an army of data engineers. Needless to say, that was not my data infrastructure.\nTo be more succinct, most of the “data infrastructure” I’ve worked with has just been csv files. And our ETL processes – cleaning csv files to output more csv files (or rds).\nBased on my very informal polling of people in the field, this is WAY more common than anyone admits, especially if you’re on a team that’s just upgrading to R. As you make that switch, it’s easy to be discouraged feeling like everyone else is lightyears ahead. The (not so) dirty secret is that they’re not really.\nIf I had to guess, the modal data scientist/analyst working in industry does ETL across a mixture of flat files and SQL databases. It’s still relatively rare to find people working on higher-power systems.1\nComing into a new role and discovering that you’re expected to wrangle a bunch of csv files can be frustrating, especially if you don’t reall ywant to be a data engineer. But you should. The number one way to be a better data scientist is to become the data engineer you wish you had.\nBeing a great junior data scientist is 75% just knowing the data super well. A junior data scientist who can confidently identify things that just look weird in data is worth approximately 47 XGBoost models and 76 convolutional neural nets.\nA data scientist who combines data science knowledge with expertise on the data’s provenance and the data-generating process is way more likely to make a good catch or identify a clever new feature for modeling than a better modeler with little understanding of the specifics of the data.\n\n\n\nSo yes, become the data engineer for your data because you need it, but also to make yourself a better data scientist. It won’t kill you.\nBest Practices for ETL on Flat Files\nSince I’ve done and managed a lot of ETL involving flat files, here are some tips I’d suggest:\nUse git to manage your code, and git LFS for managing data. Git will choke and die on data files larger than 50Mb or so, but aside from some annoyingly long download and upload times, git LFS has been a good tool for sharing data up to a few Gb across my team.\nStore all data in a data folder, with input data separated from cleaned data.\nIf feasible, create standardized cleaning functions (in a package!) so that variable names are the same across different projects.\nConsider creating a data access API in R so you call a function to access data instead of loading a csv file.2 The advantage is that it abstracts away from exactly how you store the data, so you can get used to loading your data in a simple way, and you can always update the backend to a SQL server or something else without changing the way you access your data.\nOr honetly, use cases that really need those systems. Postgres will get you pretty far.↩\nDon’t get scared by “an API” if you’re not familiar! It just means a bunch of functions so that you can access in a consistent way.↩\n",
    "preview": {},
    "last_modified": "2021-07-07T19:47:13+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-09-upgrading-to-r-2-failures-coming-get-ready/",
    "title": "Upgrading to R #2: Failure's Coming, Get Ready!",
    "description": "Prep in advance for things to be hard.",
    "author": [
      {
        "name": "Alex Gold",
        "url": {}
      }
    ],
    "date": "2019-01-23",
    "categories": [],
    "contents": "\nAs a new manager working with my team of data scientists, upgrading to R meant doing something new, and something hard. As I should’ve known, new + hard is a great recipe for mistakes and failures. I learned a lot from those mistakes - lessons that I couldn’t have anticipated ahead of time. I’ll share my surprising (and less surprising) things I learned from one particular big failure in this blog post.\nTo start, here’s an animation I made that describes Alex’s big goof-up:\n\n\n\nTL;DR: my team was working hard onboarding themselves to R, a mistake made it to a client, and I overreacted with new policies that burned a lot of my team’s goodwill. Regaining the team’s trust required eating some words, retracting some policies, and a bunch of all-team discussions to figure out how to do better in the future.\nI should’ve realized that my team’s goodwill was my most precious resource as a manager. Squandering it wasn’t a simple matter of not being nice, it (thankfully temporarily) damaged my ability to effectively lead and guide them, and diminished the likelihood that they would do their best work. Here are some things I wish I’d done ahead of time.\nShare More Context\nAs far as my teammate knew their role in this project was to create a plot in ggplot based on some data they were given. I’d shared very little about why that plot was important, where the data was coming from, or who was the intended audience. Without that knowledge, there were a multitude of ways my teammate could make totally sensible decisions that resulted in mistakes - mistakes that could’ve been avoided had I just shared more context.\nHere’s something like the plot we were making:\n\n\n\nAmong the context I should’ve shared was that we couldn’t derive the color coding of the data points ourselves.1 Since I’d neglected make sure my teammate really understood what we were doing, my teammate just computed the color themselves (metric > threshold -> green, red otherwise). This totally reasonable choice, which resulted in mistakes the client noticed, could’ve totally been avoided had I spent a few more minutes sharing more context.\nPrepare Emotionally\nThe number 1 thing I could’ve done to make this experience less bad was to avoid overreacting.2 If I could’ve gone back to a week before we found our mistake, I would’ve told past me that a mistake was inevitable, and that I should be ready to react - not unilaterally, but by rallying the team to fix the issue together.\nThe project was important, and the client seeing a mistake was bad. But draconian new procedures and rules resulted in ill-will from my team that was ultimately way more costly.\nWhat eventually got us back on track was when we met as a team, and had a few meetings in the style of a blameless postmortem figuring out where communication had broken down and how we could avoid it next time. The guidelines we eventually generated and adopted as a group were actually pretty similar to the rules I’d introduced, but this time we were actually able to stick to them, because we’d generated them together.\nPrepare Technically\nBefore this incident,I focused 100% of my mistake-avoiding attention on technical prepartion - explicit error avoidance and checking. I’ve since come to believe that error checking is an important part of the solution, but on a team that primarily produces data insights, the number one way to avoid errors is to give analysts and data scientists the knowledge they need to find mistakes themselves.\nSanity Checks\nAll data scientists know the horror of finishing an analysis, breathing a huge sigh of relief, sending it off, and then realizing that something just doesn’t add up. I’ve definitely had categories that add up to more than 100 percent, some 256 year-olds, and coefficents so big they could power a trip to the moon.3\nRemembering to do these sanity checks before anything gets sent out is hard, especially since finding something might result in more work. This is doubly true for junior team members, who might not have made all these mistakes before and might not totally know what they’re looking for.\nOne strategy that helped was removing some of the cognitive load of what to check. That meant building analysis templates that included sanity checks on results, checklists for review and execution, and R packages that automated simple checks. Removing some of the cognitive load of remembering to do all these checks was relieving for junior and senior team members alike.\nCode Review\nCode review is great. But it was easy to believe only “real” code review (i.e. comments on github merge requests) mattered. More and more, I’ve come to think that the primary purpose of code review is to trigger thinking by the code’s author. Given that the author is 100x more familiar with it (to a first approximation), I found that code review that forces the writer to rethink what they’ve done catches many mistakes, and that the venue isn’t really important.\nSince this incident, my team has gotten more consistent at using git for code reviews, but we’ve also gotten better at doing low-tech code review. Any review, no matter how low-tech, is 1,000x better than code review that didn’t happen because I was waiting for the team to “get up to speed with git”. Sometimes I even like to print out code, read through it, and mark it up like an essay. Decidedly low-tech, but really engages the mental compiler.\nTakeaways\nSo, here are the big lessons I took home from my moment of failure:\nNew + Hard -> Failure, and being emotionally ready is at least as importance as technical preparation.\nOverreacting to a mistakes as a manager can cause a bigger problem than the mistake itself.\nAlways share more context.\nTechnical solutions like templates, checklists, packages, and code review are great, but mostly because they reduce cognitive load of simple checks, or because they force the author to reengage.\nA Coda: Book Recs\nI can’t leave this blog post without sharing three book recommendations that really helped me. While the screw up I describe here is real, I wasn’t entirely without tools to cope, and these three books helped form my mental model for how we might think about recovery as a team.\nRising Strong by Brene Brown is a great exploration of failure and recovery, it provides some interesting perspectives on failure and resilience from both a personal and professional level.\nHigh Output Management by Andy Grove is a pretty traditional “management” book, and his explanation of managing to task-relevant maturity has been a helpful way to think about helping the people I manage (or helping my managers help me) avoid potential mistakes and failures.\nChecklist Manifesto by Atul Gawande is one of my all time favorite books, and it really influenced the way I think about what should be automated vs checklisted vs left up to humans.\nTo be precise, whether the difference between the metric and the threshold was statistically significant, which we didn’t have the data to compute ourselves.↩\nEspecially if you’re the team lead, but really no matter what.↩\nOr more likely 999 year-olds. Missing data should never have numeric codes, amirite?!?↩\n",
    "preview": "posts/2019-11-09-upgrading-to-r-2-failures-coming-get-ready/upgrading-to-r-2-failures-coming-get-ready_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-07-07T19:47:13+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2019-11-09-upgrading-to-r-1-excitement-skills/",
    "title": "Upgrading to R #1: Excitement >> Skills",
    "description": "You can teach skills, but not excitement.",
    "author": [
      {
        "name": "Alex Gold",
        "url": {}
      }
    ],
    "date": "2019-01-20",
    "categories": [],
    "contents": "\nThis is the first in a series of posts sharing some insights on upgrading to R. See the index post for others.\nWhether you’re a team leader trying to take your group to the next level, or an indvidual contributor just trying to infect your org with R love, finding companions is really important.\nFinding the right people will determine whether you end up part of a crack R team or fall back to whatever you’re doing now. A few factors can ease the way from the desolate R-less place you might live now back to the to the beautiful shire full of free range R code (ok, swear I’m done with LOTR references).\nHere’s my totally subjective estimate of some important factors:\n\n\n\nWired: Team’s enthusiasm for learning R\nTired: How much R they already know\nThis is pretty straightforward. By far the most important factor that will determine your team’s success in transitioning to R is whether they’re excited about it, and the least important is how much they already know. Learning R just really isn’t that hard. People who are excited to learn will do it, and those who aren’t won’t. It’s that simple.\nDon’t know who’s excited and who’s not? Ask them! And don’t just ask whether they’re excited to learn R. What you really want to know is whether they’re excited to work on a team that operates with R at its core. Transitioning to R from Excel or Stata has a bunch of potential benefits. Making those benefits real requires a lot more than just writing some code. It’s a wholesale transition in terms of how the team thinks about work (see future blog posts for more on that).\nLesser Angels\nBeyond enthusiasm, there are a couple of things you can do to make it easier, whether you’re a team member or the leader.\nTidyverse-First Orientation: If you’ve been doing R for a while, you probably know this, but the Tidyverse is AWESOME. It takes the wonderful benefits of R and adds an opinionated take on what code should look like. If you’re just learning R, start with the Tidyverse. You’ll be glad you did because it’s magical.1\nRelentless R: Adoping R as a team isn’t really about a programming langauge, it’s organizational change. Writing code is easy. Changing hearts, minds, and workflows is hard. There are people who make whole careers out of this stuff. Don’t get down if it’s slow going. I’m skeptical it’s possible to really transition a team to R in less than 12 months.\nLearning Environment: More on this during Tip #2: Failure, but you’re trying to make room for people to learn. That means it’s important to make work a space where they won’t feel stupid for asking questions or making mistakes. Hopefully this is already true for your workplace…but it’s especially important if you’re trying to get your team to do something new and hard like use R. Every workplace is busy, but almost every one can spare an hour a week to do a mini-seminar on R, or to share a package of the week. On my team, we have a show-and-tell every Friday where people share something they’ve learned. Very often, that something is a fun new R thing!\nA sub-point to this: If you can, don’t expect that all this learning will take place outside of work. Some people on your team may have kids or parents they’re caring for, or other life circumstances that mean that they have less capacity to do out-of-work learning than others. They can still be amazing partners on your journey to R. If you’re the team lead, try and make space for them to learn. If not, make sure they’re included on at-work R plans.\n\nThis presentation by MilesMcBain on the magic of R packages was FANTASTIC - one of my favorites from RStudio::conf(2019).↩\n",
    "preview": {},
    "last_modified": "2021-07-07T19:47:13+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-09-upgrading-to-r-rstudio-conf-2019/",
    "title": "Upgrading to R: RStudio Conf 2019",
    "description": "Upgrading a team to R.",
    "author": [
      {
        "name": "Alex Gold",
        "url": {}
      }
    ],
    "date": "2019-01-19",
    "categories": [],
    "contents": "\nA few months ago, I was beyond thrilled to be chosen to give an e-poster at RStudio::conf(2019) on the topic of Upgrading to R. I had such a blast talking to people who are trying to get their team to use more R, sharing some of the lessons I’ve learned over the last few years. My slides are available online, but I’d also like to share some of the things I spoke with people about, and a little on how I made the slides.\nUpgrading to R\nSo many data scientists have had the same experience - walking onto a team where something other than R or Python was the language of choice. Unless that other language was Julia or Scala, this was almost certainly a disappointing moment. But it doesn’t have a be a reason to run screaming for the hills.\nI’ve now worked on several teams that have successfully transitioned from Excel or Stata to R. These transitions, while sometimes painful, were definitely worth it. Along the way, I learned a few tips and made a lot of mistakes. I decided I’d try to share as much as possible to help anyone else in the same boat. These lessons will be particularly applicable to someone leading a data science team that’s transitioning. Some will also be relevant if you’re a junior staffer trying to push change, but not all.\nIn thinking about the transitions I’ve been part of, here are tips that might’ve helped me if I’d known them when I started. Links will be added as the posts go live.\nEdit: I seem to have fallen behind on these; maybe I’ll finish someday…\nThe Tips\nExcitement >> Skills\nFailure’s Coming, Get Ready\nYou are the Data Engineer your Team Needs\nIt Takes a Team to Write a Package\nGit: Sometimes the Right Thing isn’t the Easiest\nHave you met my Friend RMarkdown?\nDon’t Get Too Excited Just Yet\nI plan to write a blog post on each of these, plus an extra about how I put together these tips and the accompanying plots.\n\n\n",
    "preview": {},
    "last_modified": "2021-07-07T19:47:13+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-09-custom-fonts-in-ggplot2/",
    "title": "Custom fonts in ggplot2",
    "description": "Put custom fonts into plots.",
    "author": [
      {
        "name": "Alex Gold",
        "url": {}
      }
    ],
    "date": "2017-08-28",
    "categories": [],
    "contents": "\nA lot of things are great and easy to do in ggplot2…but putting new fonts in can be a pain in the butt. I had to do this last week, so I decided to write down what I did (as much for me as for anyone else).\nGet the package extrafont: install.packages('extrafont')\nStart extrafont: library(extrafont)\nImport system fonts to R (this can take a minute): font_import()\nLoad fonts: loadfonts(device = 'win'). If you’re on a Mac, just loadfonts().\nFonts are loaded!\nNow, when you type windowsFonts() in windows or , you’ll get a list of all available fonts:\n\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\n$`Agency FB`\n[1] \"Agency FB\"\n\n$Algerian\n[1] \"Algerian\"\n\n$`Arial Black`\n[1] \"Arial Black\"\n\n$Arial\n[1] \"Arial\"\n...\nNow, when you want to use a font in ggplot, it’s as easy as calling theme(family = 'Arial').\nNote that if you’re on a Mac, you won’t get the default Microsoft Office fonts, so if you’re trying to convince people that you can switch from Powerpoint by making Excel-like plot styles, you’ll need to add the fonts manually. Luckily, it’s really easy to do through the font book utility on your Mac (just type Font into spotlight), and this blog post.\n\n\n",
    "preview": {},
    "last_modified": "2021-07-07T19:47:13+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-09-rstats-nyc-2017/",
    "title": "RStats NYC 2017",
    "description": "My writeup of RStats NYC 2017.",
    "author": [
      {
        "name": "Alex Gold",
        "url": {}
      }
    ],
    "date": "2017-04-23",
    "categories": [],
    "contents": "\nFriday and Saturday of this weekend was the 2017 RStatsNYC conference. With the small exception of remarkably uncomfortable chairs, the conference was fantastic (and great food!). There was a remarkable variety in what the speakers covered – from deep thoughts on the ethics and philosophy of doing data science, to some super useful RStudio tips and tricks and everything in between. I decided to take my notes from the two days and post below in case there’s things anyone else might find useful.\nOne of the remarkable similarities between the speakers was the degree to which many talks focused on process of doing data science. It feels like there’s a lot of room for growth in terms of understanding and standardizing best practices of doing data science.\nDeep Thoughts\nMany of the speakers shared some great nuggets of insight into doing data science that extended far beyond simple lessons about R. Some of my favorites below.\nThe Past, Present, and Future of Data Science\nStatistics has gradually added different parts of the statistical process to the field — but many process points remain to be integrated. We don’t have a theory of a good statistical workflow. Meta-systematization is needed. We need to systematize how we systematize things.\nWhy do we need theory? Theory is scalable. Simulations are not.\nWe should be specifying priors about effect sizes along with research designs.\nIs having a core data science team at a company a good thing or a bad thing?\nThe history of software engineering is a great guide for the future of data science.\nAs data gets bigger, machine learning problems and questions will get bigger too. Our jobs are safe.\nThe hardest job of a data scientist is turning open-ended problems in the real world into practical problem statements that are solvable using data. Defining success is essential.\nSoftware engineers build and grow software — the sky is the limit. Data scientists are scientists and are constrained by the real world.\nData Science as Business Intelligence (and Empathy)\nAnalysis doesn’t end at result delivery — it ends at developing and proselytizing new business strategies and innovation.\nAutomating Excel workflows can be a first step towards bringing R to a team.\nAgile development tells user stories as an empathy hack.\nAs a ______\nI want ______\nSo I can ______ \nPerson-level stories (the near) are always more meaningful than data stories (the far). We need to balance both as Data Scientists.\nIn development, we need to have an actual user in mind, rather than a theoretical user who wants everything.\nHow to Live-Tweet an Event (a great aside)\nThree important elements in live tweet of event: conference hashtag #rstatsnyc, @speaker (put . first so it’s viewable publicly), always include a pic.\nThey can have one of a few purposes: intro person/topic, spread insight, link to resource, spread positivity!\nLive tweet: conference hashtag, @speaker, picture, positivity {{< tweet 855792361527541760 >}}\nHow to Hackathon\nIf you’re not at risk of failing a few days a quarter, you’re not pushing.\nKeys to focused hacking: rock solid infrastrure, minimize context switching (away messages), parallel work, strong and clear conversation, timeline accountability (detailed to the half-hour), positive team dynamic, time-boxing decisions.\nHow to start an analysis:\nIdentify assumptions about a solution\nRank assumptions by how critical they are\nTest assumptions with the simplest experiment possible\n\nFail fast. The worst thing about spending a few months barking up the wrong tree isn’t the time or money lost — it’s team demoralization.\nTruths about data science:\nNerd != won’t engage with business\nShiny tool != hard problem\nShiny tool != being a smart data scientist\n\nOpen Source and Package-Building\nThe hardest step in joining the open source community is going from 0 contributions to 1.\nOpen source contributions: failing test with fix > failing test > bug report\nRemember to include sessionInfo()\n\nPackage release ideas:\nRelease quickly, but also slowly — take time to fix dumb decisions\nBlaze new trails, but don’t reinvent the wheel\nTell people about your package — then listen.\n\nPackage design: documentation > usability > performance > features\nInteresting Research\nThere were only a few presentations of actual research projects — but the ones presented were really wonderful.\nCell tower data can reveal interesting patterns in movement — in particular return to normal flow of people after natural disasters.\nStack Overflow has a selection of data from stack overflow — can do text analysis on it. There’s also a jobs list and an api.\nIt is possible to predict which officers in police departments will have adverse incidents with the public in order to arrange interventions. Nashville and Charlottesville are doing this. This work can save lives.\nROpenSci does peer review for R packages. Many of their packages get data from scientific databases or interface with scientific equipment.\nStatistical and Programming Tools\nConvolutional Neural Net: used for image recognition\nVPNs are good privacy tools, we should probably all use one.\nR Packages to Investigate\nThere were so many neat R packages mentioned over the course of the conference. Here was a short list of ones I didn’t know and wanted to look into.\npackrat: helps you create a package template file\nplumber: allows you to easily turn regular R code into an API\nRXKCD: add XKCD cartoons to stuff!\ntrelliscope: many-panel data vis\ncompareGroups: compare demographics and other aspects across groups\nhtmlWidgets: integrate javascript applets into R code. We want interactivity, and javascript is de-facto standard for everything having to do with interactivity.\nipywidgets: similar for python\n\nbroom: turn analytics output into tidy data frames\ngoodpractice: does a variety of checking for good package development practice\nlintr: helps check for good code style\ndevtools::spellcheck()\n\nsolidify, highcharts: commonly used presentation packages\ntidytext: help turn text into tidy data frames\nGenerally Helpful Programming Hints, tips, and tricks\nrr-init: ROpenSci package to create skeleton of project\nRun R script myscript.R from command line: Rscript myscript.R\nRStudio Server runs on port 8787 by default\nAnaconda can be used to download a local R version\nCan install entire tidy verse at once: install.packages(“tidyverse”)\nAlways choose a license for your github repo. MIT is the loosest.\nCan put an empty folder in a git repo by creating and adding a file to git touch folder/.gitkeep\nmake bash — can create a makefile (make.sh) with assigned dependencies and will only run files that have been updated since last run\nJoin R mailing list\nContributing to documentation (or improving unhelpful error messages) is a great way to get started on contributing to Open Source\nIn R save() saves multiple objects with their names as .Rda. Using load restores them into the environment. saveRds() saves a single object without its name. Restoring requires assignment a <- readRds().\nRStudio Hints, Tips, Tricks\nThe RStudio folks gave a great presentation on some tricks and tips in RStudio. Some of my favorite takeaways:\nRealtime inline TeX previews in RNotebooks: $EQN$\nNotebook preview — doesn’t re-run, just renders what’s been run\nCan include code chunks that run code other than R (SQL, Bash, Python). Instead of {r} in chunk header, put (e.g.) {sql connection = con, output.var = ‘varname’} and then can put SELECT statement in body that will be assigned to varname\nIf you want to include variables in SQL code, use ?varname in code.\nRStudio git integration provides nice way to view diffs\nTab autocomplete is fuzzy — just type unique letters in command and tab for rest\nShift-Tab to insert code snippets — there are built in (tools > global options > code snippets)\nStarting to type a command and then cmd + arrow up gives history of all times that command used\nCan easily go from history to console or script\nctrl + . search files, fns, etc\nCTRL + SHIFT + . search among tabs\nctrl + shift + f (“or if that’s too hard to remember, ctrl capital F”) to customize where searching\nSHIFT + ALT + k list keyboard shortcuts\n\n\n\n",
    "preview": {},
    "last_modified": "2021-07-07T19:47:13+00:00",
    "input_file": {}
  }
]
